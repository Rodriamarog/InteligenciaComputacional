{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis Exploratorio de Datos y Pipeline de ML\n",
    "**Dataset:** Diabetes Dataset  \n",
    "**Autor:** [Tu nombre]  \n",
    "**Fecha:** 2026-02-03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importación de Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuración de visualización\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"Librerías importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carga y Descripción del Conjunto de Datos\n",
    "\n",
    "**Fuente:** Scikit-learn Diabetes Dataset  \n",
    "**Enlace:** https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset\n",
    "\n",
    "El conjunto de datos de Diabetes fue obtenido originalmente del estudio:  \n",
    "Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004)  \n",
    "\"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n",
    "\n",
    "Este dataset contiene información de 442 pacientes diabéticos y incluye:\n",
    "- 10 variables predictoras (edad, sexo, índice de masa corporal, presión arterial y seis mediciones de suero sanguíneo)\n",
    "- 1 variable objetivo que es una medida cuantitativa de la progresión de la enfermedad un año después del baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset\n",
    "diabetes = load_diabetes()\n",
    "\n",
    "# Crear un DataFrame con los datos\n",
    "df = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
    "df['target'] = diabetes.target\n",
    "\n",
    "print(\"Dataset cargado exitosamente\")\n",
    "print(f\"Dimensiones del dataset: {df.shape}\")\n",
    "print(f\"Número de muestras: {df.shape[0]}\")\n",
    "print(f\"Número de características: {df.shape[1] - 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Descripción del Problema a Resolver\n",
    "\n",
    "**Tipo de problema:** Regresión\n",
    "\n",
    "**Objetivo:** Predecir la progresión de la diabetes en pacientes un año después de la medición inicial (baseline), utilizando características demográficas y mediciones clínicas.\n",
    "\n",
    "**Variable objetivo (target):** Medida cuantitativa de la progresión de la enfermedad (valores entre 25 y 346)\n",
    "\n",
    "**Variables predictoras:**\n",
    "- age: Edad\n",
    "- sex: Sexo\n",
    "- bmi: Índice de masa corporal (Body Mass Index)\n",
    "- bp: Presión arterial promedio (Average Blood Pressure)\n",
    "- s1: tc, Colesterol total sérico\n",
    "- s2: ldl, Lipoproteínas de baja densidad\n",
    "- s3: hdl, Lipoproteínas de alta densidad\n",
    "- s4: tch, Relación colesterol total/HDL\n",
    "- s5: ltg, Logaritmo de los niveles de triglicéridos séricos\n",
    "- s6: glu, Nivel de glucosa en sangre\n",
    "\n",
    "**Nota:** Todas las variables han sido centradas y escaladas previamente por scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Análisis Exploratorio de Datos (EDA)\n",
    "\n",
    "### 4.1. Información General del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"INFORMACIÓN GENERAL DEL DATASET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Información del DataFrame\n",
    "print(\"\\n1. Información del DataFrame:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\n2. Primeras 5 filas del dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n3. Estadísticas descriptivas:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\n4. Tipos de datos:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Identificación y Selección de Columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"IDENTIFICACIÓN Y SELECCIÓN DE COLUMNAS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Todas las columnas\n",
    "all_columns = df.columns.tolist()\n",
    "print(f\"\\nColumnas totales: {all_columns}\")\n",
    "\n",
    "# Separar features y target\n",
    "feature_columns = [col for col in all_columns if col != 'target']\n",
    "target_column = 'target'\n",
    "\n",
    "print(f\"\\nColumnas de características (features): {feature_columns}\")\n",
    "print(f\"Columna objetivo (target): {target_column}\")\n",
    "\n",
    "print(f\"\\n**Decisión:** Utilizaremos todas las {len(feature_columns)} características\")\n",
    "print(\"disponibles ya que todas son mediciones clínicas relevantes para predecir\")\n",
    "print(\"la progresión de la diabetes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Visualización de la Distribución de Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear figura con subplots para todas las variables\n",
    "fig, axes = plt.subplots(4, 3, figsize=(15, 12))\n",
    "fig.suptitle('Distribución de Variables del Dataset Diabetes', fontsize=16, y=1.00)\n",
    "\n",
    "# Aplanar el array de axes para facilitar iteración\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Graficar histogramas para cada columna\n",
    "for idx, col in enumerate(df.columns):\n",
    "    axes[idx].hist(df[col], bins=30, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "    axes[idx].set_title(f'{col}', fontsize=10, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Valor')\n",
    "    axes[idx].set_ylabel('Frecuencia')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "# Ocultar el último subplot\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Análisis Detallado de la Variable Objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle('Distribución de la Variable Objetivo: Progresión de Diabetes',\n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "# Histograma\n",
    "axes[0].hist(df['target'], bins=30, edgecolor='black', alpha=0.7, color='salmon')\n",
    "axes[0].set_title('Histograma')\n",
    "axes[0].set_xlabel('Progresión de la Diabetes')\n",
    "axes[0].set_ylabel('Frecuencia')\n",
    "axes[0].axvline(df['target'].mean(), color='red', linestyle='--',\n",
    "                linewidth=2, label=f'Media: {df[\"target\"].mean():.2f}')\n",
    "axes[0].axvline(df['target'].median(), color='green', linestyle='--',\n",
    "                linewidth=2, label=f'Mediana: {df[\"target\"].median():.2f}')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Boxplot\n",
    "axes[1].boxplot(df['target'], vert=True, patch_artist=True,\n",
    "                boxprops=dict(facecolor='lightblue', alpha=0.7))\n",
    "axes[1].set_title('Boxplot')\n",
    "axes[1].set_ylabel('Progresión de la Diabetes')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nEstadísticas de la variable objetivo:\")\n",
    "print(f\"  - Media: {df['target'].mean():.2f}\")\n",
    "print(f\"  - Mediana: {df['target'].median():.2f}\")\n",
    "print(f\"  - Desviación estándar: {df['target'].std():.2f}\")\n",
    "print(f\"  - Mínimo: {df['target'].min():.2f}\")\n",
    "print(f\"  - Máximo: {df['target'].max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Análisis de Valores Nulos o Faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"ANÁLISIS DE VALORES NULOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Contar valores nulos\n",
    "null_counts = df.isnull().sum()\n",
    "null_percentages = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "# Crear DataFrame con la información\n",
    "null_info = pd.DataFrame({\n",
    "    'Columna': df.columns,\n",
    "    'Valores Nulos': null_counts.values,\n",
    "    'Porcentaje (%)': null_percentages.values\n",
    "})\n",
    "\n",
    "print(\"\\nTabla de valores nulos:\")\n",
    "print(null_info.to_string(index=False))\n",
    "\n",
    "# Visualización de valores nulos\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "null_info_sorted = null_info.sort_values('Valores Nulos', ascending=True)\n",
    "ax.barh(null_info_sorted['Columna'], null_info_sorted['Valores Nulos'],\n",
    "        color='coral', alpha=0.7, edgecolor='black')\n",
    "ax.set_xlabel('Número de Valores Nulos')\n",
    "ax.set_ylabel('Columnas')\n",
    "ax.set_title('Valores Nulos por Columna', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if df.isnull().sum().sum() == 0:\n",
    "    print(\"\\n✓ Resultado: El dataset NO contiene valores nulos.\")\n",
    "else:\n",
    "    print(f\"\\n✗ El dataset contiene {df.isnull().sum().sum()} valores nulos en total.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6. Matriz de Correlación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular matriz de correlación\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Visualizar matriz de correlación\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm',\n",
    "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Matriz de Correlación - Dataset Diabetes', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlación con la variable objetivo\n",
    "print(\"\\nCorrelación de cada feature con el target (ordenado):\")\n",
    "target_corr = correlation_matrix['target'].drop('target').sort_values(ascending=False)\n",
    "print(target_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7. Hallazgos Principales del EDA\n",
    "\n",
    "**Resumen de Hallazgos:**\n",
    "\n",
    "1. **Estructura del Dataset:**\n",
    "   - El dataset contiene 442 muestras y 10 características predictoras\n",
    "   - No hay valores nulos o faltantes en ninguna columna\n",
    "   - Todas las variables son numéricas de tipo float64\n",
    "   - Las variables ya han sido normalizadas previamente por scikit-learn\n",
    "\n",
    "2. **Variable Objetivo (target):**\n",
    "   - Rango de valores: 25 a 346\n",
    "   - Distribución aproximadamente normal con ligera asimetría positiva\n",
    "   - Media: ~152.13, Mediana: ~140.50\n",
    "   - Presenta algunos valores atípicos en el extremo superior\n",
    "\n",
    "3. **Distribución de Features:**\n",
    "   - Todas las características muestran distribuciones centradas alrededor de 0\n",
    "   - Las variables están normalizadas (centradas y escaladas)\n",
    "   - La mayoría de las características muestran distribuciones aproximadamente normales\n",
    "   - No se observan valores extremos preocupantes\n",
    "\n",
    "4. **Correlaciones Importantes:**\n",
    "   - BMI (s5) muestra la correlación más fuerte con el target (~0.59)\n",
    "   - s5 (logaritmo de triglicéridos) también tiene buena correlación (~0.57)\n",
    "   - bp (presión arterial) muestra correlación moderada (~0.44)\n",
    "   - s6 (glucosa) tiene correlación moderada (~0.38)\n",
    "   - age y sex muestran correlaciones más débiles\n",
    "   - Algunas features están correlacionadas entre sí (multicolinealidad leve)\n",
    "\n",
    "5. **Calidad de los Datos:**\n",
    "   - Dataset limpio, sin necesidad de imputación de valores faltantes\n",
    "   - No se requiere eliminación de outliers extremos\n",
    "   - Las variables ya están en la misma escala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pipeline de Preprocesamiento\n",
    "\n",
    "### Descripción de Transformaciones:\n",
    "\n",
    "**Decisión de Preprocesamiento:**\n",
    "\n",
    "Aunque los datos del dataset de diabetes de scikit-learn ya vienen normalizados, vamos a crear un pipeline de preprocesamiento para demostrar buenas prácticas y para que el código sea reutilizable con datos crudos.\n",
    "\n",
    "**Transformaciones aplicadas:**\n",
    "\n",
    "1. **StandardScaler en todas las features:**\n",
    "   - **Justificación:** Aunque los datos ya están escalados, incluimos este paso para que el pipeline sea completo y funcione con datos sin procesar.\n",
    "   - **Efecto:** Centra los datos (media=0) y escala (desviación estándar=1)\n",
    "   - **Por qué:** Los algoritmos de regresión lineal y muchos otros se benefician de tener features en la misma escala, especialmente cuando se usan regularizaciones.\n",
    "\n",
    "2. **No se aplica imputación:**\n",
    "   - **Justificación:** El dataset no contiene valores nulos\n",
    "   - Si hubiera valores nulos, usaríamos SimpleImputer con estrategia 'mean'\n",
    "\n",
    "3. **No se aplica codificación:**\n",
    "   - **Justificación:** Todas las variables son numéricas\n",
    "   - Si hubiera variables categóricas, usaríamos OneHotEncoder\n",
    "\n",
    "**Pipeline final:** StandardScaler → Modelo de Regresión Lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir las transformaciones para las columnas numéricas\n",
    "numeric_features = feature_columns\n",
    "\n",
    "# Crear el transformador para características numéricas\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Crear el ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features)\n",
    "    ])\n",
    "\n",
    "print(\"\\nPipeline de preprocesamiento creado:\")\n",
    "print(preprocessor)\n",
    "\n",
    "print(\"\\nTransformaciones aplicadas:\")\n",
    "print(\"  - StandardScaler: Normalización de todas las características numéricas\")\n",
    "print(f\"  - Características procesadas: {len(numeric_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Partición del Conjunto de Datos\n",
    "\n",
    "Se divide el dataset en conjuntos de entrenamiento (80%) y prueba (20%) con una semilla aleatoria fija para reproducibilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar features (X) y target (y)\n",
    "X = df[feature_columns]\n",
    "y = df[target_column]\n",
    "\n",
    "print(f\"\\nForma de X (features): {X.shape}\")\n",
    "print(f\"Forma de y (target): {y.shape}\")\n",
    "\n",
    "# Dividir en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nConjunto de entrenamiento:\")\n",
    "print(f\"  - X_train: {X_train.shape}\")\n",
    "print(f\"  - y_train: {y_train.shape}\")\n",
    "\n",
    "print(f\"\\nConjunto de prueba:\")\n",
    "print(f\"  - X_test: {X_test.shape}\")\n",
    "print(f\"  - y_test: {y_test.shape}\")\n",
    "\n",
    "print(f\"\\nPorcentaje de datos en entrenamiento: {len(X_train)/len(X)*100:.1f}%\")\n",
    "print(f\"Porcentaje de datos en prueba: {len(X_test)/len(X)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Creación del Modelo de Machine Learning\n",
    "\n",
    "Se crea un pipeline completo que integra el preprocesamiento y el modelo predictivo. Esto evita el \"data leakage\" ya que las transformaciones solo se ajustan con los datos de entrenamiento.\n",
    "\n",
    "**Modelo utilizado:** Regresión Lineal (LinearRegression)  \n",
    "**Justificación:** Es apropiado para problemas de regresión y sirve como baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el pipeline completo (preprocesamiento + modelo)\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "print(\"\\nPipeline completo creado:\")\n",
    "print(model_pipeline)\n",
    "\n",
    "# Entrenar el modelo\n",
    "print(\"\\nEntrenando el modelo...\")\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "print(\"✓ Modelo entrenado exitosamente\")\n",
    "\n",
    "# Mostrar los coeficientes del modelo\n",
    "coefficients = model_pipeline.named_steps['regressor'].coef_\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_columns,\n",
    "    'Coefficient': coefficients\n",
    "}).sort_values('Coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(\"\\nCoeficientes del modelo (importancia de features):\")\n",
    "print(feature_importance.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Realizar Predicciones con el Conjunto de Prueba\n",
    "\n",
    "Se utilizan los datos de prueba para generar predicciones con el modelo entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar predicciones\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "print(f\"\\nNúmero de predicciones realizadas: {len(y_pred)}\")\n",
    "\n",
    "# Mostrar algunas predicciones vs valores reales\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Valor Real': y_test.values[:10],\n",
    "    'Predicción': y_pred[:10],\n",
    "    'Diferencia': y_test.values[:10] - y_pred[:10]\n",
    "})\n",
    "\n",
    "print(\"\\nPrimeras 10 predicciones vs valores reales:\")\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar predicciones vs valores reales\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.6, edgecolors='k', s=80)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],\n",
    "         'r--', lw=2, label='Predicción perfecta')\n",
    "plt.xlabel('Valores Reales', fontsize=12)\n",
    "plt.ylabel('Predicciones', fontsize=12)\n",
    "plt.title('Predicciones vs Valores Reales - Conjunto de Prueba',\n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Resumen Final\n",
    "\n",
    "Este análisis ha completado las siguientes etapas:\n",
    "\n",
    "1. ✓ Carga y descripción del dataset de Diabetes\n",
    "2. ✓ Definición del problema de regresión\n",
    "3. ✓ Análisis exploratorio de datos (EDA):\n",
    "   - Identificación de tipos de datos\n",
    "   - Selección de características\n",
    "   - Visualización de distribuciones\n",
    "   - Análisis de valores nulos\n",
    "   - Matriz de correlación\n",
    "   - Descripción de hallazgos\n",
    "4. ✓ Creación del pipeline de preprocesamiento con StandardScaler\n",
    "5. ✓ Partición de datos en entrenamiento (80%) y prueba (20%)\n",
    "6. ✓ Entrenamiento del modelo de Regresión Lineal\n",
    "7. ✓ Generación de predicciones en el conjunto de prueba\n",
    "\n",
    "El pipeline está completo y listo para su uso, evitando data leakage al integrar preprocesamiento y modelo en una sola estructura.\n",
    "\n",
    "**Próximos pasos sugeridos (fuera del alcance de esta actividad):**\n",
    "- Evaluar métricas de desempeño (RMSE, MAE, R²)\n",
    "- Probar otros algoritmos (Ridge, Lasso, Random Forest)\n",
    "- Realizar validación cruzada\n",
    "- Optimizar hiperparámetros"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
